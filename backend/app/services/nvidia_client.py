import httpx
import logging
import numpy as np
from typing import Dict, Any, List
from app.core.config import get_settings

settings = get_settings()
logger = logging.getLogger(__name__)

class NvidiaClient:
    """
    è¦æ ¼æ›¸ 1.2: è³‡æºèª¿åº¦ç­–ç•¥èˆ‡ API å°è£
    è² è²¬èˆ‡ NVIDIA NIM æœå‹™æºé€šã€‚
    åŒ…å« Error 500 é˜²ç¦¦æ©Ÿåˆ¶ (Soft Landing) èˆ‡ Mock é™ç´šã€‚
    """
    def __init__(self):
        self.llm_api_key = settings.NVIDIA_LLM_API_KEY
        self.embedding_api_key = settings.NVIDIA_EMBEDDING_API_KEY
        self.base_url = settings.LLM_API_URL
        self.model = settings.LLM_MODEL_NAME
        self.embed_model = settings.EMBEDDING_MODEL_NAME
        
    def _get_headers(self, api_key: str) -> Dict[str, str]:
        return {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json",
            "Accept": "application/json"
        }

    async def get_embedding(self, text: str) -> List[float]:
        """
        ç²å–å‘é‡ (å«ï¼šç©ºå€¼æ””æˆª + éŒ¯èª¤è»Ÿè‘—é™¸ + Mock é™ç´š)
        """
        # ğŸ›¡ï¸ ç¬¬ä¸€é“é˜²ç·šï¼šç©ºå€¼æ””æˆª (Input Sanitization)
        if not text or not str(text).strip():
            logger.warning(f"[NvidiaClient] Embedding input is empty or None. Returning random vector.")
            return list(np.random.rand(1024))

        url = f"{self.base_url}/embeddings"
        payload = {
            "input": [text],
            "model": self.embed_model,
            "input_type": "query",
            "encoding_format": "float"
        }
        
        headers = self._get_headers(self.embedding_api_key)
        
        # ğŸ›¡ï¸ ç¬¬äºŒé“é˜²ç·šï¼šAPI éŒ¯èª¤è»Ÿè‘—é™¸ (Soft Landing)
        try:
            async with httpx.AsyncClient(timeout=30.0) as client: # Embedding å¿«ï¼Œ30s è¶³å¤ 
                response = await client.post(url, json=payload, headers=headers)
                response.raise_for_status()
                data = response.json()
                return data['data'][0]['embedding']

        except Exception as e:
            # ğŸ›¡ï¸ ç¬¬ä¸‰é“é˜²ç·šï¼šMock é™ç´š (Fallback)
            logger.error(f"[NvidiaClient] Embedding API failed for: '{str(text)[:20]}...'. Reason: {str(e)}")
            logger.warning(f"[NvidiaClient] System falling back to Mock Vector to prevent crash.")
            
            # å›å‚³éš¨æ©Ÿå‘é‡ï¼Œè®“æµç¨‹èƒ½ç¹¼çºŒèµ°ä¸‹å»
            return list(np.random.rand(1024))

    async def generate_completion(self, 
                                messages: List[Dict[str, str]], 
                                temperature: float = 0.2,
                                max_tokens: int = 4096) -> str:
        """
        èª¿ç”¨ LLM ç”Ÿæˆå›æ‡‰ (å« Timeout å„ªåŒ–)
        """
        url = f"{self.base_url}/chat/completions"
        payload = {
            "model": self.model,
            "messages": messages,
            "temperature": temperature,
            "top_p": 0.7,
            "max_tokens": max_tokens,
            "stream": False
        }

        headers = self._get_headers(self.llm_api_key)

        try:
            async with httpx.AsyncClient(timeout=120.0) as client: # âš ï¸ å»¶é•·è‡³ 120 ç§’
                response = await client.post(url, json=payload, headers=headers)
                response.raise_for_status()
                data = response.json()
                content = data['choices'][0]['message']['content']
                return content
        except Exception as e:
            logger.error(f"[NvidiaClient] LLM Generation Failed: {str(e)}")
            raise e